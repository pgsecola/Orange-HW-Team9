library(dplyr)
library(doBy)
library(bindr)
library(ggplot2)
library(histogram)
library(stringr)
library(tidyverse)
library(tidyr)
library(readxl)
library(tibble)
library(SASxport)
library(reshape2)
library(haven)
library(fma)
library(tseries)
library(fma)
library(tseries)
library(expsmooth)
library(lmtest)
library(zoo)
library(seasonal)
library(lubridate)
library(data.table)
library(caret)
library(corrplot)
library(shiny)
library(splitstackshape)
library(gutenbergr)
library(text2vec)
library(tidytext)
library(wordcloud)
library(SnowballC)
library(ggmap)
library(factoextra)
library(mclust)

###########Hierarchical Clustering#######

#Create Word Sentiment Score
#Alter Variable Types

nrc_total <- get_sentiments("afinn")
listings <- read.csv("C:\\Users\\pseco\\Downloads\\boston-airbnb-open-data\\listings.csv")
reviews <- read.csv("C:\\Users\\pseco\\Downloads\\boston-airbnb-open-data\\reviews.csv")
reviews$comments <- as.character(reviews$comments)
reviews$reviewer_name <- as.character(reviews$reviewer_name)
reviews$date <- as.Date(reviews$date, "%Y-%m-%d")

glimpse(reviews)

listings_imprtnt <- select(listings, c("id","host_is_superhost",
                       "neighbourhood_cleansed","latitude","longitude",
                       "property_type", "room_type","price", "weekly_price", "monthly_price",
                       "availability_365","review_scores_rating","review_scores_accuracy", "review_scores_value",
                       "reviews_per_month"))

#Group reviews by their respective listing ids 
#and extract those listings that have greater than 4 reviews

rv <- reviews %>% group_by(listing_id) %>%
  count(listing_id, sort = TRUE) %>% filter( n >= 4) %>% select(-"n")

#break up the dataset's listing into individual words
#join this dataset with rv, keeping only those listing IDs that have 4 or more
# reviews remove all of the na values

new_reviews <- reviews %>%
  group_by(listing_id) %>%
  unnest_tokens(word, comments)  %>%
  right_join(rv,by="listing_id") %>% filter(!is.na(word)) %>%
  left_join(nrc_total,by="word") %>% filter(!is.na(score)) 


#Find the number of words scored

score         <- new_reviews %>% group_by(listing_id) %>% mutate(sscore = sum(score)) %>% distinct(listing_id,sscore)
nwords        <- new_reviews %>% group_by(listing_id) %>% count(listing_id) 

#compute Avg score for each listing and Standardize the Score

complete <- nwords %>% left_join(score,"listing_id") %>% mutate(avg = sscore/n)

complete$avg <- scale(complete$avg) #standardize the score

#Change variable from "id" to "listing_id" in listings table

names(listings)[1] <- "listing_id"

#Join Listings and Review Tables by Listing_id and Scale Long & Lat

combined <- complete %>% left_join(listings,"listing_id")
combined$std.lat <- scale(combined$latitude)
combined$std.lon <- scale(combined$longitude)

#create Matrix with Standardized "avg score", "Long", "Lat" variables

toC<- cbind(combined$avg,combined$std.lat,combined$std.lon)

#####KMeans Clustering##########
#use toC matrix to determine the appropiate number of clusters

fviz_nbclust(toC, kmeans, method="wss") #can't tell
fviz_nbclust(toC, kmeans, method="silhouette") #4 cluster appropiate

#perform KMeans analysis using optimal number of clusters (4)
#clusters for listing sentiment, longitude, latitude
#Output KMeans object

km_clust <- kmeans(toC,4,nstart = 25)

#create Data Frame with location data and cluster information

Boston_cluster_table <- data.frame(Listing_clust = km_clust$centers[,1],
                                   Latitude_clust = km_clust$centers[,2],
                                   Longitude_clust = km_clust$centers[,3])


